{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:13:44.920146Z",
     "start_time": "2024-04-14T15:10:11.883151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from utils import get_completion\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kreimben/CodeMind-gemma\", padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"kreimben/CodeMind-gemma\")"
   ],
   "id": "36aec76683d0dcd0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c36dc75303f43358849c2b56949f5dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksid\\PycharmProjects\\CodeMind\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aksid\\.cache\\huggingface\\hub\\models--kreimben--CodeMind-gemma. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0470bc5d98234201831036ed04b57fce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/522 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae7c3b939d8d4bfb9bba96a4e27f0cec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c848f491e6948b6a411807aeafc416b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78ae19e8fef04a1fafecc99a98a49c63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81e75c4f66f44bebb616156a5d3f9eef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ecdf868a92c4a47be3fd6a056c05a47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45a9b657819649d4bb02ff89b4c8f38a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5825f65dbc8a4898a544dd24d2aa4ffc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2e52a6faa504fbd9583ebd90b2140ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:13:46.915814Z",
     "start_time": "2024-04-14T15:13:44.921146Z"
    }
   },
   "cell_type": "code",
   "source": "model = model.to('cuda')",
   "id": "ecc2426b050a5323",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:17:38.032181Z",
     "start_time": "2024-04-14T15:17:12.677770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "res = get_completion(\n",
    "    'i dont know about word break problem. please give me an approch about this problem in python.',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    # device='cpu',\n",
    ")"
   ],
   "id": "bd87970e0dbd3d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.9 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:17:38.047309Z",
     "start_time": "2024-04-14T15:17:38.033182Z"
    }
   },
   "cell_type": "code",
   "source": "answer = res.split('model')[-1]",
   "id": "4eb3d31eb0a2ce3a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:17:38.062549Z",
     "start_time": "2024-04-14T15:17:38.048309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "answer = answer.replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "display(Markdown(answer))"
   ],
   "id": "fc4a98a822d76d5f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": " Given a dictionary and a word, find all rhymes. You can choose any letter to find the rhyme for it. Two words are considered close if the absolute difference in their lexicographical sum is less than or equal to 3. Analyze the word break problem in Python. We have to first get the word and the dictionary. Then we will calculate the total word score by iterating over the words in the dictionary and summing them up.  For each word, we will split it into the individual characters for better analysis of the lexicographical sum. The algorithm will be similar to this. The algorithm will be similar to this. \nLet's see the implementation. \n```python\ndef wordBreak(words, chars):\n    dp = [False] * len(words)\n\n    # initial state 0-indexed\n    dp[0] = True\n\n    # now iterate over and add edges to each dp cell\n    for i in range(1, len(words)):\n        for j in range(i):\n            if words[i] == words[j][-1] and j+1 in dp:\n                dp[i] = True\n\n    # find the path from 0 to len-1\n    # find the path from 0 to len-1\n    ansp = \"\"\n    k = len(words) - 1\n    while k >= 0:\n        if not dp[k]:\n            break\n        ansp += words[k]\n        k -= 1\n\n    return ansp[::-1]\n```\nThis solution runs in O(n * m) time and uses O(n) additional space.\nLet's observe the steps, we find the optimal solution to the above problem, when the given string s is the empty string:  What about if the given string is composed of exactly one word in the dictionary, should the optimal solution be the same as the simple version of the above problem?  Certainly!  The key to solve this problem is to split the given word in such a way that we will just take the last word or simply the last character of the split word.  For the first split of the word, it is obvious that we have to take the last word as that would be the word.  Suppose we are given this word in \"word Break\" 6 characters:  Now the character at index 0 of the word, which is \"B \" and the last character of it, which is \"E \", are not matched and we take \"W \" in the first split, since they are not the same, so the problem is not clear, now for first 2 characters \"W \" and \"F \" at index 0 and 2 of the \"W \" are not matched and we take \"E \" in the first split, we take \"W \" in the second split, now it is clear.  The third problem is that if we use the character \"W \" and \"E \" then the last character of the \"W \" would be not matched but in the first and second splits, the last characters match with \"W \".  Therefore,  We can observe that in this problem the same sub strings take only one more position at index 0, we take the whole thing for our first split and leave the position at index 0 for next iteration, as we are going to do the two remaining sub strings for that, the character at the last index of the string, namely that \"E \", \"D \", and \"B \" would be not matched so we take the corresponding characters and leave the position for next iteration, let's take the \"D \" and we take \"B \", for the problem of 3 character we are keeping this character, for the next iteration, now, for the first character of the second split, we will take the \"B \" so that would be \"B \" and \"E \", we leave the position before that to next iteration but now for the \"I \", we will take \"L \", so, for the position 2, we will take \"B \", before that, we will take \"L \", which means that we will take them both in our first iteration.  The third sub word from sub word of B and E, we take 1 more position \"D \" so for our first two sub words, we take them \"B \", \"D \", \"E \", now our two splits of the string are such  1. B D E 2. B D\nNow, let's keep analyzing all the elements in the given word, and our goal is that both elements for first split and our chosen character (we use \"L \" and \"W \") have to be matched and not the characters, we take them all and now, how we will do the 3rd split, we take \"L \" and \"E \", then the position before that to next iteration is not taken so we will take the \"D \" and"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:14:12.854857Z",
     "start_time": "2024-04-14T15:14:12.840602Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "eab8d30f3b5f5c56",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
