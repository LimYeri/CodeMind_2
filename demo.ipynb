{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T10:30:32.449114Z",
     "start_time": "2024-04-12T10:30:25.881159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from utils import get_completion\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kreimben/CodeMind-gemma\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"kreimben/CodeMind-gemma\")"
   ],
   "id": "36aec76683d0dcd0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acc83efbae734c0486ccdfeba624c3ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T10:30:34.424989Z",
     "start_time": "2024-04-12T10:30:32.450115Z"
    }
   },
   "cell_type": "code",
   "source": "model = model.to('cuda')",
   "id": "ecc2426b050a5323",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T10:30:48.691335Z",
     "start_time": "2024-04-12T10:30:34.425989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "get_completion(\n",
    "    'i dont know about word break problem. please give me a approch about this problem.',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "id": "bd87970e0dbd3d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "C:\\Users\\aksid\\PycharmProjects\\CodeMind\\venv\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:573: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"user Below is an instruction that describes a task. Write a response that appropriately completes the request. i dont know about word break problem. please give me a approch about this problem.\\nmodel model# Intuition\\\\n<!-- Describe your first thoughts on how to solve this problem. -->\\\\n\\\\n# Approach\\\\n<!-- Describe your approach to solving the problem. -->\\\\n\\\\n# Complexity\\\\n- Time complexity:\\\\n<!-- Add your time complexity here, e.g. $$O(n)$$ -->\\\\n\\\\n- Space complexity:\\\\n<!-- Add your space complexity here, e.g. $$O(n)$$ -->\\\\n\\\\n# Code\\\\n```\\\\nclass Solution {\\\\npublic:\\\\n    int canBreak(string s) {\\\\n        vector<int> dp(s.size(), 0);\\\\n        vector<int> can(s.size(), 0);\\\\n        dp[0] = 1;\\\\n        can[0] = 1;\\\\n        for(int i = 1; i < s.size(); i++) {\\\\n            for(int j = 0; j < i; j++) {\\\\n                if(s[j] == s[i] && dp[j] == 1) {\\\\n                    can[i] = 1;\\\\n        }else if(i - j > 1 && can[j] == 1) {\\\\n                    can[i] = 1;\\\\n        }\\\\n            }\\\\n            dp[i] = can[i];\\\\n        }\\\\n\\\\n        return dp[s.size() - 1];\\\\n    }\\\\n};\\\\n``` model# Intuition\\\\n\\\\nwe have to check if any substring(starting from index j) of the given string(length s.size()) can form complete word/string or not.\\\\n<!-- Describe your first thoughts on how to solve this problem. -->\\\\n\\\\n# Approach\\\\n- start iterating till s.size() from last element\\\\n- declare a **boolean array yes of size s.size() which tells what\\\\'s the last index from which a complete word can form**\\\\n- find the last index from last which can form a complete word.\\\\n- now start iterating from s[i] to s[j-1] backward in the given string\\\\n- if at any point word can form or word is already formed it\\\\'s complete check you can break this loop and then insert the true value in the yes array\\\\n- if yes is true for any substring means substring can\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T10:30:48.706842Z",
     "start_time": "2024-04-12T10:30:48.692335Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4eb3d31eb0a2ce3a",
   "outputs": [],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
